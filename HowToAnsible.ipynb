{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba1d6410",
   "metadata": {},
   "source": [
    "# Ansible Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de4e411",
   "metadata": {},
   "source": [
    "How to configure a cluster, install programs like hadoop and spark and run commands remotely on the cluster using Ansible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcef5b8e",
   "metadata": {},
   "source": [
    "#### Setting up machines:\n",
    "\n",
    "First we create the configuration files:\n",
    "\n",
    "<u> inventory\n",
    "\n",
    "[workers]\n",
    "\n",
    "10.195.6.170 ansible_user=ubuntu <br>\n",
    "10.195.6.170 ansible_python_interpreter=/usr/bin/python3\n",
    "\n",
    "[master]\n",
    "    \n",
    "10.195.6.121 ansible_user=ubuntu <br>\n",
    "10.195.6.121 ansible_python_interpreter=/usr/bin/python3\n",
    "\n",
    "<u> ansible.cfg\n",
    "\n",
    "[defaults]\n",
    "\n",
    "inventory = inventory <br>\n",
    "remote_user = ubuntu <br>\n",
    "private_key_file = ~/.ssh/id_rsa <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9068382e",
   "metadata": {},
   "source": [
    "#### To check correct setup:\n",
    "$ ansible all -m ping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f5c1fa",
   "metadata": {},
   "source": [
    "#### Adding mapping for /etc/hosts:\n",
    "<u> setup_hosts.yml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bccfe0a4",
   "metadata": {},
   "source": [
    "- hosts: all\n",
    "  tags: all\n",
    "  tasks:\n",
    "         - name: Add mappings to /etc/hosts file\n",
    "           become: yes\n",
    "           become_user: root\n",
    "           copy:\n",
    "                   dest: /etc/hosts\n",
    "                   content: |\n",
    "                            127.0.0.1 localhost\n",
    "                            192.168.3.113 node1\n",
    "                            192.168.3.130 node2\n",
    "                            # The following lines are desirable for IPv6 capable hosts\n",
    "                            ::1 ip6-localhost ip6-loopback\n",
    "                            fe00::0 ip6-localnet\n",
    "                            ff00::0 ip6-mcastprefix\n",
    "                            ff02::1 ip6-allnodes\n",
    "                            ff02::2 ip6-allrouters\n",
    "                            ff02::3 ip6-allhosts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7da42",
   "metadata": {},
   "source": [
    "$ ansible-playbook --ask-become-pass setup_hosts.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92936e80",
   "metadata": {},
   "source": [
    "#### Setting up ssh connection\n",
    "\n",
    "<u> connections.yml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3e8912e6",
   "metadata": {},
   "source": [
    "- name: SSH Connection - Master\n",
    "  hosts: master\n",
    "  tags: ssh, master\n",
    "  vars_files:\n",
    "         - external_vars.yml\n",
    "  tasks:\n",
    "         - name: generate key pair\n",
    "           tags: run\n",
    "           shell: ssh-keygen -t rsa -N \"\"  -f ~/.ssh/id_rsa\n",
    "           args:\n",
    "                   creates: \"~/.ssh/id_rsa\"\n",
    "         - name: Fetch public key of master\n",
    "           fetch:\n",
    "                   src: \"~/.ssh/id_rsa.pub\"\n",
    "                   dest: \"files/id_rsa.pub\"\n",
    "                   flat: yes\n",
    "         - name: Fetch private key of master\n",
    "           fetch:\n",
    "                   src: \"~/.ssh/id_rsa\"\n",
    "                   dest: \"files/id_rsa\"\n",
    "                   flat: yes\n",
    "         - name: Update authorized_hosts\n",
    "           shell: \"cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\"\n",
    "         - name: Add Nodes to known hosts master\n",
    "           tags: known_hosts_master\n",
    "           shell: ssh-keyscan -H { { item.ip } } >> ~/.ssh/known_hosts\n",
    "           with_items:\n",
    "                - \"{ { nodes } }\"\n",
    "- name: SSH Connection - Worker Nodes\n",
    "  hosts: workers\n",
    "  tags: ssh, workers\n",
    "  vars_files:\n",
    "      - external_vars.yml\n",
    "  tasks:\n",
    "      - name: Copy the public key to worker nodes\n",
    "        copy:\n",
    "           src: files/id_rsa.pub\n",
    "           dest: ~/.ssh/id_rsa.pub\n",
    "      - name: Copy the private key to worker nodes\n",
    "        copy:\n",
    "           src: files/id_rsa\n",
    "           dest: ~/.ssh/id_rsa\n",
    "           mode: 0400\n",
    "      - name: Update authorized_hosts\n",
    "        shell: \"cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\"\n",
    "      - name: Add Nodes to known hosts worker\n",
    "        tags: known_hosts, workers\n",
    "        shell: ssh-keyscan -H { {item.ip} } >> ~/.ssh/known_hosts\n",
    "        with_items:\n",
    "            - \"{ {nodes} }\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "89ccbd3e",
   "metadata": {},
   "source": [
    "where external_vars.yml looks like:\n",
    "\n",
    "- nodes:\n",
    "       - {hostname: node1, ip: 192.168.3.113}\n",
    "       - {hostname: node2, ip: 192.168.3.130}\n",
    "- master_node:\n",
    "       - {hostname: node1, ip: 192.168.3.113}\n",
    "- worker_nodes:\n",
    "       - {hostname: node2, ip: 192.168.3.130}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4667b735",
   "metadata": {},
   "source": [
    "$ ansible-playbook --ask-become-pass connections.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a09909",
   "metadata": {},
   "source": [
    "#### Install Hadoop and Spark:\n",
    "\n",
    "<u> install_programs.yml\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6be34a2c",
   "metadata": {},
   "source": [
    "---\n",
    "- name: Installation of Java and Hadoop\n",
    "  hosts: all\n",
    "  tags: all\n",
    "  vars_files:\n",
    "         - external_vars.yml\n",
    "  tasks:\n",
    "         - name: Install Java\n",
    "           become_user: root\n",
    "           become: yes\n",
    "           apt:\n",
    "                   name: openjdk-8-jdk\n",
    "                   update_cache: yes\n",
    "         - name: Add Java to bashrc\n",
    "           blockinfile:\n",
    "                   path: ~/.bashrc\n",
    "                   block: |\n",
    "                           export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "                           export PATH=$PATH:$JAVA_HOME/bin\n",
    "                   marker: \"# {mark} ANSIBLE MANAGED BLOCK JAVA\"\n",
    "         - name: Install unzip\n",
    "           become_user: root\n",
    "           become: yes\n",
    "           package:\n",
    "                name: unzip\n",
    "         - name: Install Hadoop\n",
    "           unarchive:\n",
    "                   src: https://archive.apache.org/dist/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz\n",
    "                   dest: /home/ubuntu\n",
    "                   remote_src: yes\n",
    "         - name: Add Hadoop to bashrc\n",
    "           blockinfile:\n",
    "                   path: ~/.bashrc\n",
    "                   block: |\n",
    "                           export HADOOP_HOME=/home/ubuntu/hadoop-2.7.3\n",
    "                           export HADOOP_CONF_DIR=$HOME/hadoop-2.7.3/etc/hadoop/\n",
    "                           export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin\n",
    "                   marker: \"# {mark} ANSIBLE MANAGED BLOCK HADOOP\"\n",
    "         - name: Install Spark\n",
    "           unarchive:\n",
    "                   src: https://downloads.apache.org/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
    "                   dest: /home/ubuntu\n",
    "                   remote_src: yes\n",
    "         - name: Add Spark to bashrc\n",
    "           blockinfile:\n",
    "                   path: ~/.bashrc\n",
    "                   block: |\n",
    "                           export SPARK_HOME=$HOME/spark-3.1.1-bin-hadoop2.7\n",
    "                           export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "                   marker: \"# {mark} ANSIBLE MANAGED BLOCK SPARK\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae65636",
   "metadata": {},
   "source": [
    "\n",
    "$ ansible-playbook --ask-become-pass install_programs.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a236bca",
   "metadata": {},
   "source": [
    "<U> configure_hadoop.yml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49484171",
   "metadata": {},
   "source": [
    "\n",
    "- name: For master\n",
    "  hosts: master\n",
    "  tags: hadoop_master_configuration\n",
    "  vars_files:\n",
    "         - external_vars.yml\n",
    "  tasks:\n",
    "         - name: Edit core-site.xml\n",
    "           blockinfile:\n",
    "                   path: ~/hadoop-2.7.3/etc/hadoop/core-site.xml\n",
    "                   insertafter: <configuration>\n",
    "                   block: |\n",
    "                           <property>\n",
    "                           <name>fs.default.name</name>\n",
    "                           <value>hdfs://node1:9000</value>\n",
    "                           </property>\n",
    "                   marker: \"\"\n",
    "         - name: Copy the template of mapred-site.xml\n",
    "           shell: cp ~/hadoop-2.7.3/etc/hadoop/mapred-site.xml.template ~/hadoop-2.7.3/etc/hadoop/mapred-site.xml\n",
    "         - name: Edit mapred-site.xml\n",
    "           blockinfile:\n",
    "                   path: ~/hadoop-2.7.3/etc/hadoop/mapred-site.xml\n",
    "                   insertafter: <configuration>\n",
    "                   block: |\n",
    "                           <property>\n",
    "                           <name>mapred.job.tracker</name>\n",
    "                           <value>node1:54311</value>\n",
    "                           <description>The host and port that the MapReduce job tracker runs\n",
    "                           at.  If \"local\", then jobs are run in-process as a single map\n",
    "                           and reduce task.\n",
    "                           </description>\n",
    "                           </property>\n",
    "                           <property>\n",
    "                           <name>mapred.child.java.opts</name>\n",
    "                           <value>-Xmx1024m</value>\n",
    "                           </property>\n",
    "                   marker: \"\"\n",
    "         - name: Edit hdfs-site.xml\n",
    "           blockinfile:\n",
    "                   path: ~/hadoop-2.7.3/etc/hadoop/hdfs-site.xml\n",
    "                   insertafter: <configuration>\n",
    "                   block: |\n",
    "                           <property>\n",
    "                           <name>dfs.replication</name>\n",
    "                           <value>1</value>\n",
    "                           </property>\n",
    "                           <property>\n",
    "                           <name>dfs.namenode.name.dir</name>\n",
    "                           <value>file:/usr/local/hadoop_tmp/hdfs/namenode</value>\n",
    "                           </property>\n",
    "                   marker: \"\"\n",
    "         - name: Ansible check directory.\n",
    "           stat:\n",
    "                   path: /usr/local/hadoop_tmp/hdfs/namenode\n",
    "           register: namenode_folder\n",
    "         - name: Configure hdfs directory if not exists\n",
    "           become_user: root\n",
    "           become: yes\n",
    "           shell: |\n",
    "                   mkdir -p /usr/local/hadoop_tmp/hdfs/namenode\n",
    "                   chown ubuntu:ubuntu -R /usr/local/hadoop_tmp/\n",
    "                   chmod 777 /usr/local/hadoop_tmp/hdfs/namenode/\n",
    "           when: namenode_folder.stat.exists == false\n",
    "         - name: Edit yarn-site.xml\n",
    "           blockinfile:\n",
    "                   path: ~/hadoop-2.7.3/etc/hadoop/yarn-site.xml\n",
    "                   insertafter: <configuration>\n",
    "                   block: |\n",
    "                           <property>\n",
    "                           <name>yarn.nodemanager.aux-services</name>\n",
    "                           <value>mapreduce_shuffle</value>\n",
    "                           </property>\n",
    "                           <property>\n",
    "                           <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n",
    "                           <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n",
    "                           </property>\n",
    "                           <property>\n",
    "                           <name>yarn.resourcemanager.resource-tracker.address</name>\n",
    "                           <value>node1:8025</value>\n",
    "                           </property>\n",
    "                           <property>\n",
    "                           <name>yarn.resourcemanager.scheduler.address</name>\n",
    "                           <value>node1:8030</value>\n",
    "                          </property>\n",
    "                           <property>\n",
    "                           <name>yarn.resourcemanager.address</name>\n",
    "                           <value>node1:8050</value>\n",
    "                           </property>\n",
    "                           <property>\n",
    "                           <name>yarn.resourcemanager.webapp.address</name>\n",
    "                           <value>node1:8088</value>\n",
    "                           </property>\n",
    "                           <property>\n",
    "                           <name>yarn.app.mapreduce.am.staging-dir</name>\n",
    "                           <value>/tmp</value>\n",
    "                           </property>\n",
    "                   marker: \"\"\n",
    "         - name: Add to master file\n",
    "           blockinfile:\n",
    "                   path: ~/hadoop-2.7.3/etc/hadoop/masters\n",
    "                   create: true\n",
    "                   block: |\n",
    "                           Template:Item.hostname\n",
    "                   marker: \"#{mark} ANSIBLE MANAGED BLOCK  Template:Item.hostname\"\n",
    "           with_items:\n",
    "                        - \"Template:Master node\"\n",
    "         - name: Add to slaves files\n",
    "           blockinfile:\n",
    "                   path: ~/hadoop-2.7.3/etc/hadoop/slaves\n",
    "                   create: true\n",
    "                   block: |\n",
    "                           Template:Item.hostname\n",
    "                   marker: \"#{mark} ANSIBLE MANAGED BLOCK  Template:Item.hostname\"\n",
    "           with_items:\n",
    "                        - \"Template:Worker nodes\"\n",
    "         - name: change Spark-env.config\n",
    "           copy:\n",
    "                   dest: ~/spark-3.1.1-bin-hadoop2.7/conf/spark-env.sh\n",
    "                   force: no\n",
    "                   content: |\n",
    "                           export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "                           export SPARK_WORKER_CORES=2\n",
    "         - name: change Spark-env.slaves\n",
    "           blockinfile:\n",
    "                   path: ~/spark-3.1.1-bin-hadoop2.7/conf/slaves\n",
    "                   create: true\n",
    "                   block: |\n",
    "                           Template:Item.hostname\n",
    "                           <property>\n",
    "                           <name>yarn.resourcemanager.address</name>\n",
    "                           <value>node1:8050</value>\n",
    "                           </property>\n",
    "                           <property>\n",
    "                           <name>yarn.resourcemanager.webapp.address</name>\n",
    "                           <value>node1:8088</value>\n",
    "                           </property>\n",
    "                   marker: \"\"\n",
    "         - name: Add to master file\n",
    "           blockinfile:\n",
    "                   path: ~/hadoop-2.7.3/etc/hadoop/masters\n",
    "                   create: true\n",
    "                   block: |\n",
    "                           Template:Item.hostname\n",
    "                                       marker: \"#{mark} ANSIBLE MANAGED BLOCK Template:Item.hostname\"\n",
    "           with_items: \"Template:Worker nodes\"\n",
    "         - name: set JAVA_HOME environment variable\n",
    "           action: lineinfile dest=~/hadoop-2.7.3/etc/hadoop/hadoop-env.sh regexp='export JAVA_HOME.*' line='export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64'\n",
    "         - name: Copy configuration files to all workers\n",
    "           tags: scp\n",
    "           shell: scp ~/hadoop-2.7.3/etc/hadoop/masters ~/hadoop-2.7.3/etc/hadoop/slaves ~/hadoop-2.7.3/etc/hadoop/core-site.xml  ~/hadoop-2.7.3/etc/hadoop/mapred-site.xml ~/hadoop-2.7.3/etc/hadoop/yarn-site.xml ~/hadoop-2.7.3/etc/hadoop/hadoop-env.sh ubuntu@Template:Item.ip:~/hadoop-2.7.3/etc/hadoop/.\n",
    "           with_items: \"Template:Worker nodes\"\n",
    "         - name: Copy spark configuration files to all workers\n",
    "           tags: scp_spark\n",
    "           shell: scp ~/spark-3.1.1-bin-hadoop2.7/conf/spark-env.sh ~/spark-3.1.1-bin-hadoop2.7/conf/slaves ubuntu@Template:Item.ip:~/spark-3.1.1-bin-hadoop2.7/conf/.\n",
    "           with_items: \"Template:Worker nodes\"\n",
    "- name: For workers\n",
    "\n",
    " hosts: workers\n",
    " tags: hadoop_worker_configuration\n",
    " vars_files:\n",
    "      - external_vars.yml\n",
    " tasks:\n",
    "      - name: Edit worker hdfs-site.xml\n",
    "        blockinfile:\n",
    "             path: ~/hadoop-2.7.3/etc/hadoop/hdfs-site.xml\n",
    "             insertafter: <configuration>\n",
    "             block: |\n",
    "                    <property>\n",
    "                    <name>dfs.replication</name>\n",
    "                    <value>1</value>\n",
    "                    </property>\n",
    "                    <property>\n",
    "                    <name>dfs.datanode.name.dir</name>\n",
    "                    <value>file:/usr/local/hadoop_tmp/hdfs/datanode</value>\n",
    "                    </property>\n",
    "             marker: \"\"\n",
    "      - name: Ansible check datanode directory\n",
    "        stat:\n",
    "             path: /usr/local/hadoop_tmp/hdfs/datanode\n",
    "        register: datanode_folder\n",
    "      - name: Configure worker hdfs directory if not exists\n",
    "        become_user: root\n",
    "        become: yes\n",
    "        shell: |\n",
    "               mkdir -p /usr/local/hadoop_tmp/hdfs/datanode\n",
    "               chown ubuntu:ubuntu -R /usr/local/hadoop_tmp/\n",
    "               chmod 777 /usr/local/hadoop_tmp/hdfs/datanode/\n",
    "        when: datanode_folder.stat.exists == false\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94843d7b",
   "metadata": {},
   "source": [
    "$ ansible-playbook configure_hadoop.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d9e42",
   "metadata": {},
   "source": [
    "#### Adding a new machine\n",
    "\n",
    "1. Update inventory\n",
    "\n",
    "2. Update external_vars.yml\n",
    "\n",
    "3. Update setup_hosts.yml\n",
    "\n",
    "4. Run following playbooks:\n",
    "\n",
    "$ ansible-playbook --ask-become-pass setup_hosts.yml\n",
    "\n",
    "$ ansible-playbook --ask-become-pass connections.yml\n",
    "\n",
    "$ ansible-playbook --limit <new machine's IP> install_programs.yml\n",
    " eg: ansible-playbook --limit 10.195.6.167 install_programs.yml\n",
    "\n",
    "$ ansible-playbook configure_hadoop.yml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a858033f",
   "metadata": {},
   "source": [
    "#### Running hadoop\n",
    "\n",
    "(NOTE: TO BE ONLY DONE ONCE IN THE BEGINNING WHEN CONFIGURING THE CLUSTER)\n",
    "\n",
    "<U> namenode_hdfs_format.yaml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e47487e",
   "metadata": {},
   "source": [
    "- name: To format hdfs, used on master\n",
    "  hosts: master\n",
    "  tags: hadoop_master_configuration\n",
    "  tasks:\n",
    "         - name: Format namenode\n",
    "           tags: format\n",
    "           shell: ~/hadoop-2.7.3/bin/hdfs namenode -format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d566ea",
   "metadata": {},
   "source": [
    "$ ansible-playbook namenode_hdfs_format.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400b00c4",
   "metadata": {},
   "source": [
    "<u> start_hadoop_spark.yml"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4031077b",
   "metadata": {},
   "source": [
    "- name: start hadoop and spark cluster\n",
    "  hosts: master\n",
    "  tags: start_master\n",
    "  tasks:\n",
    "         - name: Start dfs\n",
    "           shell: ~/hadoop-2.7.3/sbin/start-dfs.sh\n",
    "         - name: Start yarn\n",
    "           shell: ~/hadoop-2.7.3/sbin/start-yarn.sh\n",
    "         - name: Start Spark\n",
    "           shell: ./spark-3.1.1-bin-hadoop2.7/sbin/start-all.sh\n",
    "         - name: Check if running\n",
    "           shell: jps\n",
    "           register: result\n",
    "         - debug:\n",
    "               var: result.stdout_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0bf661",
   "metadata": {},
   "source": [
    "$ ansible-playbook start_hadoop_spark.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e2b9e4",
   "metadata": {},
   "source": [
    "#### Check status of all Hadoop Nodes:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "60aeaa6f",
   "metadata": {},
   "source": [
    "- name: Checking status of workers\n",
    "  hosts: all\n",
    "  gather_facts: no\n",
    "  tasks:\n",
    "         - name: Check if running\n",
    "           shell: jps\n",
    "           register: result\n",
    "         - debug:\n",
    "                 var: result.stdout_lines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
